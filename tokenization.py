# -*- coding: utf-8 -*-
"""tokenization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oNwLGYWUVCDMJ0-YVatLfSczAhCjtEuX
"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

texts=["I love  deep learning",
       "RNNs are powerful for sequence data",
       "Tokenization is the first step"]

tokenizer=Tokenizer()

tokenizer.fit_on_texts(texts)
sequences=tokenizer.texts_to_sequences(texts)

padded_sequences=pad_sequences(sequences,padding="pre")

print("word index:\n",tokenizer.word_index)
print("sequences:\n",sequences)
print("padded sequences:\n",padded_sequences)